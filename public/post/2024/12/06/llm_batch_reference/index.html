<!DOCTYPE html>
<html lang="zh">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>LLM_batch_reference | logn</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-N7FW2QDYM7"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-N7FW2QDYM7');
  </script>
  
  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">POSTS</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">LLM_batch_reference</span></h1>

<h2 class="date">2024/12/06</h2>
</div>

<main>
<p>[toc]</p>
<h1 id="llama-31-70b的部署和批量推理离线推理">Llama-3.1-70b的部署和批量推理（离线推理）</h1>
<p>offline inferencem, or batch inference</p>
<h3 id="top_p-vs-top_k">top_p vs top_k</h3>
<blockquote>
<p><code>top_p</code>是指在生成文本时，选择概率最高的 <code>p</code>个token作为候选集。例如，如果 <code>top_p</code>为0.9，则意味着在生成文本时，选择概率最高的90%的token作为候选集。</p>
<p><code>top_p</code>的作用是：</p>
<ul>
<li>限制生成的文本的多样性：通过选择概率最高的token，<code>top_p</code>可以限制生成的文本的多样性，使得生成的文本更加集中和可预测。</li>
<li>提高生成的文本的质量：通过选择概率最高的token，<code>top_p</code>可以提高生成的文本的质量，使得生成的文本更加流畅和自然。</li>
</ul>
<p><code>top_k</code>是指在生成文本时，选择前 <code>k</code>个概率最高的token作为候选集。例如，如果 <code>top_k</code>为100，则意味着在生成文本时，选择前100个概率最高的token作为候选集。</p>
<p><code>top_k</code>的作用是：</p>
<ul>
<li>提高生成的文本的多样性：通过选择前 <code>k</code>个概率最高的token，<code>top_k</code>可以提高生成的文本的多样性，使得生成的文本更加丰富和多样。</li>
<li>降低生成的文本的质量：通过选择前 <code>k</code>个概率最高的token，<code>top_k</code>可以降低生成的文本的质量，使得生成的文本更加随机和不确定。</li>
</ul>
<p><code>top_p</code>和 <code>top_k</code>是两个相关但不同的参数。<code>top_p</code>限制了生成的文本的多样性，而 <code>top_k</code>提高了生成的文本的多样性。通常情况下，<code>top_p</code>和 <code>top_k</code>会被同时使用，以便在生成的文本的质量和多样性之间找到一个平衡。</p>
<p>例如，如果你想生成一个高质量的文本，你可以设置 <code>top_p</code>为0.9和 <code>top_k</code>为100。这意味着在生成avour时，选择概率最高的90%的token作为候选集，并从候选集中选择前100个概率最高的token作为生成的文本。</p>
</blockquote>
<h2 id="huggingface-model">Huggingface model</h2>
<pre><code class="language-powershell">huggingface-cli login
huggingface-cli download -h
huggingface-cli download meta-llama/Llama-3.1-70B-Instruct
</code></pre>
<h2 id="vllm">Vllm</h2>
<h3 id="cuda和pytorch">CUDA和Pytorch</h3>
<blockquote>
<p><a href="https://blog.csdn.net/null_one/article/details/129412159">显卡驱动CUDA 和 pytorch CUDA 之间的区别_cuda版本和torch.cuda一样吗-CSDN博客</a></p>
</blockquote>
<p>区别了nvcc nvidia-smi torch.__version__</p>
<h3 id="conda和pip">conda和pip</h3>
<blockquote>
<p><a href="https://www.cnblogs.com/Li-JT/p/14024034.html">conda install和pip install区别 - lmqljt - 博客园</a></p>
</blockquote>
<p>pip 包含build conda一般是可执行</p>
<ol>
<li>安装环境 此处安装的是老版vllm 支持cuda11.8</li>
</ol>
<pre><code class="language-powershell"># conda 虚拟环境
conda create myenv

# 先安装pytorch
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# Install vLLM with CUDA 11.8.
export VLLM_VERSION=0.6.1.post1
export PYTHON_VERSION=310
pip install https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu11
</code></pre>
<ol start="2">
<li>代码
注意这里的要指定 <code>CUDA_VISIBLE_DEVICES</code>， 而且在代码内支持着一种方式</li>
</ol>
<blockquote>
<p><a href="https://blog.csdn.net/m0_65814643/article/details/143882882">【杂记】vLLM如何指定GPU单卡离线推理_vllm指定gpu-CSDN博客</a></p>
</blockquote>
<pre><code class="language-python">from vllm import LLM, SamplingParams
import os
os.environ[&quot;CUDA_DEVICE_ORDER&quot;] = &quot;PCI_BUS_ID&quot;
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;2&quot;

# Sample prompts.
prompts = [
    &quot;Hello, my name is&quot;,
    &quot;The president of the United States is&quot;,
    &quot;The capital of France is&quot;,
    &quot;The future of AI is&quot;,
]
# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

# Create an LLM.
llm = LLM(model=&quot;facebook/opt-125m&quot;)
# Generate texts from the prompts. The output is a list of RequestOutput objects
# that contain the prompt, generated text, and other information.
outputs = llm.generate(prompts, sampling_params)
# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f&quot;Prompt: {prompt!r}, Generated text: {generated_text!r}&quot;)
</code></pre>
<h3 id="问题一-vllm-flash-attn">问题一 <code>vllm-flash-attn</code></h3>
<p>软硬件</p>
<pre><code>NVIDIA RTX A6000
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Wed_Jun__2_19:15:15_PDT_2021
Cuda compilation tools, release 11.4, V11.4.48
Build cuda_11.4.r11.4/compiler.30033411_0
2.4.0+cu118
11.8
</code></pre>
<p>Cannot use FlashAttention-2 backend because the vllm_flash_attn package is not found. <code>pip install vllm-flash-attn</code> for better performance.</p>
<blockquote>
<p>这里安装的的不是cuda11.8的，识别不了，我找不到11.8的 <code>vllm-flash-attn</code>，遂放弃。</p>
</blockquote>
<p>ValueError: XFormers does not support attention logits soft capping.</p>
<p>重开一个2.4 cuda12.1的虚拟环境,12.1可以装vllm, 但是我本地nvidia-smi是12.0的不清楚会不会出错</p>
<blockquote>
<p>没问题，但是又变成2.5.1了，是啥情况
2.5.1+cu124</p>
</blockquote>
<pre><code class="language-shell">conda install pytorch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 pytorch-cuda=12.1 -c pytorch -c nvidia
</code></pre>
<pre><code class="language-python"># 单卡版本
from vllm import LLM, SamplingParams
import os
os.environ[&quot;CUDA_DEVICE_ORDER&quot;] = &quot;PCI_BUS_ID&quot;
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;2&quot;

prompts = [
    &quot;Hello, my name is&quot;,
    &quot;The president of the United States is&quot;,
    &quot;The capital of France is&quot;,
    &quot;The future of AI is&quot;,
]
# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

# Create an LLM.
llm = LLM(model=&quot;/path/gemma-2-9b-it&quot;)
# Generate texts from the prompts. The output is a list of RequestOutput objects
# that contain the prompt, generated text, and other information.
outputs = llm.generate(prompts, sampling_params)
# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f&quot;Prompt: {prompt!r}, Generated text: {generated_text!r}&quot;)


# 多卡版本
from vllm import LLM, SamplingParams
import os
os.environ[&quot;CUDA_DEVICE_ORDER&quot;] = &quot;PCI_BUS_ID&quot;
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0,2&quot;
tensor_parallel_size=2
prompts = [
    &quot;Hello, my name is&quot;,
    &quot;The president of the United States is&quot;,
    &quot;The capital of France is&quot;,
    &quot;The future of AI is&quot;,
]
# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

# Create an LLM.
llm = LLM(model=&quot;/path/gemma-2-9b-it&quot;, tensor_parallel_size=tensor_parallel_size)
# Generate texts from the prompts. The output is a list of RequestOutput objects
# that contain the prompt, generated text, and other information.
outputs = llm.generate(prompts, sampling_params)
# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f&quot;Prompt: {prompt!r}, Generated text: {generated_text!r}&quot;)
</code></pre>
<h3 id="问题二-gpu资源占用问题">问题二 GPU资源占用问题</h3>
<p>资源占用问题</p>
<blockquote>
<p><a href="https://www.cnblogs.com/theseventhson/p/18315598">LLM大模型：推理优化-vLLM显存使用优化 - 第七子007 - 博客园</a></p>
</blockquote>
<pre><code>speed input: 306.32 toks/s, output: 620.85 toks/s]

双卡
加载模型权重时占用了 8.6535 GB 的内存。
内存使用情况：
初始内存使用量：9.10 GiB (第一个进程) 和 23.18 GiB (第二个进程)
峰值 Torch 内存使用量：8.97 GiB (第一个进程) 和 11.02 GiB (第二个进程)
内存使用量（在内存分析后）：9.17 GiB (第一个进程) 和 23.25 GiB (第二个进程)
非 Torch 内存使用量：0.50 GiB (第一个进程) 和 14.58 GiB (第二个进程)
GPU 内存使用量：47.54 GiB
GPU 资源占用：
GPU 内存使用率：0.90
GPU 并发度：26.19x
CUDA 图形资源占用：
CUDA 图形注册数量：2975
CUDA 图形捕获所需时间：38 秒
CUDA 图形捕获所需内存：1.46 GiB

speed input: 226.13 toks/s, output: 461.36 toks/s
单卡
内存占用：
加载模型权重时占用了 17.2179 GB 的内存。
内存使用情况：
初始内存使用量：17.54 GiB
峰值 Torch 内存使用量：19.57 GiB
内存使用量（在内存分析后）：17.55 GiB
非 Torch 内存使用量：0.33 GiB
GPU 内存使用量：47.54 GiB
GPU 资源占用：
GPU 内存使用率：0.90
GPU 并发度：17.43x
CUDA 图形资源占用：
CUDA 图形捕获所需时间：19 秒
CUDA 图形捕获所需内存：1.34 GiB
与之前的输出相比，这个程序的资源占用情况有一些变化：

加载模型权重时占用的内存增加了（从 8.6535 GB 到 17.2179 GB）。
峰值 Torch 内存使用量增加了（从 11.02 GiB 到 19.57 GiB）。
CUDA 图形捕获所需时间减少了（从 38 秒到 19 秒）。
CUDA 图形捕获所需内存减少了（从 1.46 GiB 到 1.34 GiB）。

增加问题数目，双卡
加载模型权重时占用了 8.6535 GB 的内存。
内存使用情况：
初始内存使用量：9.10 GiB (第一个进程) 和 23.18 GiB (第二个进程)
峰值 Torch 内存使用量：8.97 GiB (第一个进程) 和 11.02 GiB (第二个进程)
内存使用量（在内存分析后）：9.17 GiB (第一个进程) 和 23.25 GiB (第二个进程)
非 Torch 内存使用量：0.50 GiB (第一个进程) 和 14.58 GiB (第二个进程)
GPU 内存使用量：47.54 GiB
GPU 资源占用：
GPU 内存使用率：0.90
GPU 并发度：26.19x
CUDA 图形资源占用：
CUDA 图形注册数量：2975
CUDA 图形捕获所需时间：33 秒
CUDA 图形捕获所需内存：1.46 GiB
与之前的输出相比，这个程序的资源占用情况有一些变化：

加载模型权重时占用的内存没有变化（仍然是 8.6535 GB）。
峰值 Torch 内存使用量没有变化（仍然是 8.97 GiB 和 11.02 GiB）。
CUDA 图形捕获所需时间增加了（从 19 秒到 33 秒）。
CUDA 图形捕获所需内存没有变化（仍然是 1.46 GiB）。

增加问题数目，双卡
内存占用：
加载模型权重时占用了 17.2179 GB 的内存。
内存使用情况：
初始内存使用量：17.54 GiB
峰值 Torch 内存使用量：19.57 GiB
内存使用量（在内存分析后）：17.55 GiB
非 Torch 内存使用量：0.33 GiB
GPU 内存使用量：47.54 GiB
GPU 资源占用：
GPU 内存使用率：0.90
GPU 并发度：17.43x
GPU 块数：4463
CPU 块数：780
CUDA 图形资源占用：
CUDA 图形捕获所需时间：21 秒
CUDA 图形捕获所需内存：1.34 GiB
与之前的输出相比，这个程序的资源占用情况有一些变化：

加载模型权重时占用的内存增加了（从 8.6535 GB 到 17.2179 GB）。
峰值 Torch 内存使用量增加了（从 11.02 GiB 到 19.57 GiB）。
CUDA 图形捕获所需时间减少了（从 33 秒到 21 秒）。
CUDA 图形捕获所需内存减少了（从 1.46 GiB 到 1.34 GiB）。
</code></pre>
<p><img src="image/index/1733556896187.png" alt="1733556896187"></p>
<p><img src="image/index/1733556914512.png" alt="1733556914512"></p>
<h2 id="llama-factory">Llama factory</h2>
<p>webui board</p>

</main>

  <footer>
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<script src="//cdn.jsdelivr.net/combine/npm/katex/dist/katex.min.js,npm/katex/dist/contrib/auto-render.min.js,npm/@xiee/utils/js/render-katex.js" defer></script>

<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.min.js" defer></script>

  
  <hr/>
  Thanks to © <a href="https://yihui.org">Yihui Xie</a> 2017 &ndash; 2024 | <a href="https://github.com/yihui">Github</a> | <a href="https://twitter.com/xieyihui">Twitter</a>
  
  </footer>
  </body>
</html>

